{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://colab.research.google.com/drive/1F_RNcHzTfFuQf-LeKvSlud6x7jXYkG31#scrollTo=goRmGIRI5cfC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://localhost:8080/notebooks/git/product-category/notebooks/prep_20210304A1.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "prfx_prp = 'prep_20210304A1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME = \"/data/git/product-category\"\n",
    "p_out = f'{HOME}/data/transformer_20210305D1'\n",
    "!mkdir -p {p_out}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz = 100000\n",
    "DATA2USE = f'{HOME}/data/data_sample_{sz}__{prfx_prp}.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eda "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 9)\n",
      "CPU times: user 20.5 ms, sys: 2.34 ms, total: 22.8 ms\n",
      "Wall time: 21.4 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>description</th>\n",
       "      <th>title</th>\n",
       "      <th>brand</th>\n",
       "      <th>feature</th>\n",
       "      <th>asin</th>\n",
       "      <th>domain</th>\n",
       "      <th>txt</th>\n",
       "      <th>is_validation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>639</th>\n",
       "      <td>Electronics|Car &amp; Vehicle Electronics|Car Elec...</td>\n",
       "      <td>PAPAGO! ST200-US Suction Mount for GoSafe 200 ...</td>\n",
       "      <td>PAPAGO! ST200-US Windshield Suction Cup Mount ...</td>\n",
       "      <td>PAPAGO</td>\n",
       "      <td>Excellent for mounting the PAPAGO! GoSafe 200 ...</td>\n",
       "      <td>B00WANG0FE</td>\n",
       "      <td>Electronic</td>\n",
       "      <td>PAPAGO! ST200-US Windshield Suction Cup Mount ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>Clothing, Shoes &amp; Jewelry|Women|Contemporary &amp;...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ROBERTO CAVALLI Women's Sunglasses</td>\n",
       "      <td>ROBERTO CAVALLI</td>\n",
       "      <td>Acetate frame\\nGlass lens\\nPolarization\\nLens ...</td>\n",
       "      <td>B00GGM0AKG</td>\n",
       "      <td>Clothing_Shoes_and_Jewelry</td>\n",
       "      <td>ROBERTO CAVALLI Women's Sunglasses ROBERTO CAV...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720</th>\n",
       "      <td>Electronics|Headphones</td>\n",
       "      <td>Fonus retractable earphones with a standard 3....</td>\n",
       "      <td>Fonus Black Retractable 3.5mm Mono Handsfree H...</td>\n",
       "      <td>Fonus</td>\n",
       "      <td>FONUS(TM) Brand Premium Retractable Mono Hands...</td>\n",
       "      <td>B00J42D8WC</td>\n",
       "      <td>Electronic</td>\n",
       "      <td>Fonus Black Retractable 3.5mm Mono Handsfree H...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              category  \\\n",
       "639  Electronics|Car & Vehicle Electronics|Car Elec...   \n",
       "474  Clothing, Shoes & Jewelry|Women|Contemporary &...   \n",
       "720                             Electronics|Headphones   \n",
       "\n",
       "                                           description  \\\n",
       "639  PAPAGO! ST200-US Suction Mount for GoSafe 200 ...   \n",
       "474                                                NaN   \n",
       "720  Fonus retractable earphones with a standard 3....   \n",
       "\n",
       "                                                 title            brand  \\\n",
       "639  PAPAGO! ST200-US Windshield Suction Cup Mount ...           PAPAGO   \n",
       "474                 ROBERTO CAVALLI Women's Sunglasses  ROBERTO CAVALLI   \n",
       "720  Fonus Black Retractable 3.5mm Mono Handsfree H...            Fonus   \n",
       "\n",
       "                                               feature        asin  \\\n",
       "639  Excellent for mounting the PAPAGO! GoSafe 200 ...  B00WANG0FE   \n",
       "474  Acetate frame\\nGlass lens\\nPolarization\\nLens ...  B00GGM0AKG   \n",
       "720  FONUS(TM) Brand Premium Retractable Mono Hands...  B00J42D8WC   \n",
       "\n",
       "                         domain  \\\n",
       "639                  Electronic   \n",
       "474  Clothing_Shoes_and_Jewelry   \n",
       "720                  Electronic   \n",
       "\n",
       "                                                   txt  is_validation  \n",
       "639  PAPAGO! ST200-US Windshield Suction Cup Mount ...            0.0  \n",
       "474  ROBERTO CAVALLI Women's Sunglasses ROBERTO CAV...            1.0  \n",
       "720  Fonus Black Retractable 3.5mm Mono Handsfree H...            1.0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# df = pd.read_csv(f'../data/data__{prfx_prp}.csv')\n",
    "# df = pd.read_csv(f'{HOME}/data/data_sample__{prfx_prp}.csv')\n",
    "df = pd.read_csv(DATA2USE, nrows=1000)\n",
    "print(df.shape)\n",
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.924"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.txt.notna().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(i2dmn), len(i2cat) 22 8\n",
      "Arts_Crafts_and_Sewi|Automotive|Book|CDs_and_Vinyl|Cell_Phones_and_Accessorie|Clothing_Shoes_and_Jewelry|Electronic|Grocery_and_Gourmet_Food|Home_and_Kitche|Industrial_and_Scientific|Kindle_Store|Magazine_Subscripti|Movies_and_TV|Musical_Instrument|Office_Product|Patio_Lawn_and_Garde|Pet_Supplie|Software|Sports_and_Outdoor|Tools_and_Home_Improvement|Toys_and_Game|Video_Game\n",
      "\n",
      "Automotive|Books|Clothing|Clothing, Shoes & Jewelry|Home & Kitchen|Men|Sports & Outdoors|Women\n"
     ]
    }
   ],
   "source": [
    "MIN_CNT = 50\n",
    "dmn2cnt = Counter(df.domain.value_counts().to_dict())\n",
    "i2dmn = sorted(dmn2cnt.keys())\n",
    "dmn2i = {v:k for k,v in enumerate(i2dmn)}\n",
    "cat2cnt = Counter((j for i in df.category.apply(lambda x: x.split('|')) for j in i))\n",
    "i2cat = sorted(k for k,v in cat2cnt.items() if v>50)\n",
    "cat2i = {v:k for k,v in enumerate(i2cat)}\n",
    "\n",
    "print(\"len(i2dmn), len(i2cat)\", len(i2dmn), len(i2cat))\n",
    "print(\"|\".join(i2dmn))\n",
    "print()\n",
    "print(\"|\".join(i2cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser\n",
    "import pytorch_lightning as pl\n",
    "from transformers.optimization import AdamW\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "def mk_tensors(txt, tokenizer, max_seq_length):\n",
    "    tok_res = tokenizer(\n",
    "        txt, truncation=True, padding='max_length', max_length=max_seq_length\n",
    "    )\n",
    "    input_ids = tok_res[\"input_ids\"]\n",
    "    attention_mask = tok_res[\"attention_mask\"]\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_mask = torch.tensor(attention_mask)\n",
    "    return input_ids, attention_mask\n",
    "\n",
    "def mk_ds(txt, tokenizer, max_seq_length, ys):\n",
    "    input_ids, attention_mask = mk_tensors(txt, tokenizer, max_seq_length)\n",
    "    return TensorDataset(input_ids, \n",
    "                         attention_mask, \n",
    "                         torch.tensor(ys)) \n",
    "\n",
    "class PCDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, \n",
    "                 model_name_or_path, \n",
    "                 max_seq_length, \n",
    "                 min_products_for_category,\n",
    "                 train_batch_size,\n",
    "                 val_batch_size,\n",
    "                 data_file_path=None,\n",
    "                 dataframe=None):\n",
    "        super().__init__()\n",
    "        self.data_file_path = data_file_path\n",
    "        self.dataframe = dataframe\n",
    "        self.min_products_for_category = min_products_for_category\n",
    "        self.model_name_or_path = model_name_or_path\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.train_batch_size = train_batch_size\n",
    "        self.val_batch_size = val_batch_size\n",
    "        self.num_classes = None\n",
    "      \n",
    "    def prepare_data(self):\n",
    "        #prepare_data is called from a single process (e.g. GPU 0). Do not use it to assign state (self.x = y).\n",
    "        _ = AutoTokenizer.from_pretrained(self.model_name_or_path)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name_or_path)\n",
    "        if self.dataframe is None:\n",
    "            self.dataframe = pd.read_csv(self.data_file_path)\n",
    "        self.dataframe = self.dataframe[self.dataframe.txt.notna()].copy()\n",
    "        cats = self.dataframe.category.apply(lambda x: x.split('|'))\n",
    "        cat2cnt = Counter((j for i in cats for j in i))\n",
    "        i2cat = sorted(k for k,v in cat2cnt.items() if v>self.min_products_for_category)\n",
    "        cat2i = {v:k for k,v in enumerate(i2cat)}\n",
    "        self.num_classes = len(i2cat)\n",
    "        self.i2cat, self.cat2i = i2cat, cat2i\n",
    "        \n",
    "        ys = np.zeros((len(self.dataframe), len(i2cat)))\n",
    "        for i,cats in enumerate(self.dataframe.category):\n",
    "            idx_pos = [cat2i[cat] for cat in cats.split('|') if cat in cat2i]\n",
    "            ys[i,idx_pos] = 1\n",
    "        \n",
    "        msk_val = self.dataframe.is_validation==1\n",
    "        self.df_trn = self.dataframe[~msk_val]\n",
    "        self.df_val = self.dataframe[msk_val]\n",
    "        idx_trn = np.where(~msk_val)[0]\n",
    "        idx_val = np.where(msk_val)[0]\n",
    "        self.ys_trn, self.ys_val = ys[idx_trn], ys[idx_val]\n",
    "        \n",
    "        txt = self.dataframe.txt.values\n",
    "        self.train_dataset = mk_ds(list(self.df_trn.txt), self.tokenizer, self.max_seq_length, self.ys_trn)\n",
    "        self.eval_dataset  = mk_ds(list(self.df_val.txt), self.tokenizer, self.max_seq_length, self.ys_val)\n",
    "        \n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.train_batch_size,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.eval_dataset,\n",
    "            batch_size=self.val_batch_size,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModel\n",
    "\n",
    "\n",
    "def getaccu(logits, ys):\n",
    "    return ((torch.sigmoid(logits)>0.5).int()==ys).float().mean()\n",
    "\n",
    "class PCModel(pl.LightningModule):\n",
    "    def __init__(self, model_name_or_path, num_classes, learning_rate, adam_beta1, adam_beta2, adam_epsilon):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model_name_or_path = model_name_or_path\n",
    "        self.bert = AutoModel.from_pretrained(self.model_name_or_path)\n",
    "        self.num_classes = num_classes\n",
    "        self.W = nn.Linear(self.bert.config.hidden_size, self.num_classes)\n",
    "\n",
    "    def prepare_data(self):\n",
    "        #prepare_data is called from a single process (e.g. GPU 0). Do not use it to assign state (self.x = y).\n",
    "        _ = AutoModel.from_pretrained(self.model_name_or_path)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        h = self.bert(input_ids, attention_mask)['last_hidden_state']\n",
    "        h_cls = h[:, 0]\n",
    "        return self.W(h_cls)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids, attention_mask, ys = batch    \n",
    "        logits = self(input_ids, attention_mask)\n",
    "        loss = F.binary_cross_entropy_with_logits(logits, ys)\n",
    "        accu = getaccu(logits, ys)\n",
    "        self.log('train_loss', loss, on_epoch=True)\n",
    "        self.log('train_accu', accu, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids, attention_mask, ys = batch    \n",
    "        logits = self(input_ids, attention_mask)\n",
    "        loss = F.binary_cross_entropy_with_logits(logits, ys)\n",
    "        accu = getaccu(logits, ys)\n",
    "        self.log('valid_loss', loss, on_step=False, sync_dist=True)\n",
    "        self.log('valid_accu', accu, on_step=False, sync_dist=True)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.parameters(),\n",
    "                          self.hparams.learning_rate,\n",
    "                          betas=(self.hparams.adam_beta1,\n",
    "                                 self.hparams.adam_beta2),\n",
    "                          eps=self.hparams.adam_epsilon,)\n",
    "        return optimizer    \n",
    "    \n",
    "    @staticmethod\n",
    "    def add_model_specific_args(parent_parser):\n",
    "        parser = ArgumentParser(parents=[parent_parser], add_help=False)\n",
    "        parser.add_argument('--learning_rate', type=float, default=5e-5)\n",
    "        parser.add_argument('--adam_beta1', type=float, default=0.9)\n",
    "        parser.add_argument('--adam_beta2', type=float, default=0.999)\n",
    "        parser.add_argument('--adam_epsilon', type=float, default=1e-8)\n",
    "        return parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = ArgumentParser()\n",
    "\n",
    "parser.add_argument('--model_name_or_path', type=str,\n",
    "                    default=\"distilbert-base-cased\")\n",
    "parser.add_argument('--max_seq_length', type=int, default=128)\n",
    "parser.add_argument('--min_products_for_category', type=int, default=100)\n",
    "parser.add_argument('--train_batch_size', type=int, default=16)\n",
    "parser.add_argument('--val_batch_size', type=int, default=8)\n",
    "\n",
    "parser = pl.Trainer.add_argparse_args(parser)\n",
    "parser = PCModel.add_model_specific_args(parser)\n",
    "\n",
    "args = parser.parse_args([\n",
    "    '--default_root_dir', p_out,\n",
    "])\n",
    "\n",
    "\n",
    "data_module = PCDataModule(\n",
    "    model_name_or_path=args.model_name_or_path,\n",
    "#     data_file_path=f'{HOME}/data/data_sample__{prfx_prp}.csv',\n",
    "    data_file_path=DATA2USE,\n",
    "    min_products_for_category=args.min_products_for_category,\n",
    "    max_seq_length=args.max_seq_length,\n",
    "    train_batch_size=args.train_batch_size,\n",
    "    val_batch_size=args.val_batch_size,\n",
    ")\n",
    "\n",
    "data_module.prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda3/envs/product-category/lib/python3.7/site-packages/pytorch_lightning/core/datamodule.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_prepared_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-187224dc696c>\u001b[0m in \u001b[0;36msetup\u001b[0;34m(self, stage)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mtxt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtxt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmk_ds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf_trn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtxt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mys_trn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_dataset\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mmk_ds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtxt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mys_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-187224dc696c>\u001b[0m in \u001b[0;36mmk_ds\u001b[0;34m(txt, tokenizer, max_seq_length, ys)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmk_ds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmk_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     return TensorDataset(input_ids, \n\u001b[1;32m     22\u001b[0m                          \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-187224dc696c>\u001b[0m in \u001b[0;36mmk_tensors\u001b[0;34m(txt, tokenizer, max_seq_length)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmk_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     tok_res = tokenizer(\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mtxt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'max_length'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     )\n\u001b[1;32m     13\u001b[0m     \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtok_res\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda3/envs/product-category/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2345\u001b[0m                 \u001b[0mreturn_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2346\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2347\u001b[0;31m                 \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2348\u001b[0m             )\n\u001b[1;32m   2349\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda3/envs/product-category/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mbatch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2530\u001b[0m             \u001b[0mreturn_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2531\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2532\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2533\u001b[0m         )\n\u001b[1;32m   2534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda3/envs/product-category/lib/python3.7/site-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m             \u001b[0mis_pretokenized\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_split_into_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         )\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/data/anaconda3/envs/product-category/lib/python3.7/site-packages/transformers/tokenization_utils_fast.py\u001b[0m(385)\u001b[0;36m_batch_encode_plus\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    383 \u001b[0;31m            \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    384 \u001b[0;31m            \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 385 \u001b[0;31m            \u001b[0mis_pretokenized\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_split_into_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    386 \u001b[0;31m        )\n",
      "\u001b[0m\u001b[0;32m    387 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> u\n",
      "> \u001b[0;32m/data/anaconda3/envs/product-category/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m(2532)\u001b[0;36mbatch_encode_plus\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m   2530 \u001b[0;31m            \u001b[0mreturn_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   2531 \u001b[0;31m            \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m-> 2532 \u001b[0;31m            \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   2533 \u001b[0;31m        )\n",
      "\u001b[0m\u001b[0;32m   2534 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> u\n",
      "> \u001b[0;32m/data/anaconda3/envs/product-category/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m(2347)\u001b[0;36m__call__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m   2345 \u001b[0;31m                \u001b[0mreturn_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   2346 \u001b[0;31m                \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m-> 2347 \u001b[0;31m                \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   2348 \u001b[0;31m            )\n",
      "\u001b[0m\u001b[0;32m   2349 \u001b[0;31m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> u\n",
      "> \u001b[0;32m<ipython-input-7-187224dc696c>\u001b[0m(11)\u001b[0;36mmk_tensors\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m      9 \u001b[0;31m\u001b[0;32mdef\u001b[0m \u001b[0mmk_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     10 \u001b[0;31m    tok_res = tokenizer(\n",
      "\u001b[0m\u001b[0;32m---> 11 \u001b[0;31m        \u001b[0mtxt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'max_length'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     12 \u001b[0;31m    )\n",
      "\u001b[0m\u001b[0;32m     13 \u001b[0;31m    \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtok_res\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> type(txt)\n",
      "<class 'list'>\n",
      "ipdb> len(txt)\n",
      "85895\n",
      "ipdb> txt[0]\n",
      "'Beautiful Babies: The ART of Reborn Doll Making Michele Barrow-Blisle  '\n",
      "ipdb> foo=tokenizer(txt[:1000], truncation=True, padding='max_length', max_length=max_seq_length)\n",
      "*** TypeError: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
      "ipdb> foo=tokenizer(txt[:100], truncation=True, padding='max_length', max_length=max_seq_length)\n",
      "*** TypeError: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
      "ipdb> foo=tokenizer(txt[:10], truncation=True, padding='max_length', max_length=max_seq_length)\n",
      "*** TypeError: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
      "ipdb> foo=tokenizer(txt[:1], truncation=True, padding='max_length', max_length=max_seq_length)\n",
      "ipdb> foo=tokenizer(txt[:2], truncation=True, padding='max_length', max_length=max_seq_length)\n",
      "ipdb> foo=tokenizer(txt[:5], truncation=True, padding='max_length', max_length=max_seq_length)\n",
      "ipdb> txt[:10]\n",
      "['Beautiful Babies: The ART of Reborn Doll Making Michele Barrow-Blisle  ', 'Sesame St Baby-Boys Infant 2 Pack Sesame T-Shirt with An All Over Print T-Shirt Sesame Street The perfect 2 pack tee with a long and short sleeve elmo and cookie monster tee. Both tees have soft hand feel 100% Cotton\\nImported\\nMachine Wash\\nSoft hand feel\\nGreat gift\\nPackage Dimensions:\\n                    \\n8.5 x 8.1 x 1.2 inches\\nShipping Weight:\\n                    \\n4 ounces', 'Joyful Flying Lion Ornament, 09-01301 Patience Brewster Krinkles PATIENCE BREWSTER JOYFUL FLYING LION ORNAMENT 2009. Designed by Patience Brewster. approx 7.5 made of stone resin. Comes with tags and gift box. This beautiful krinkle would also make a wonderful gift. 7.5 inch stone resin ornament\\n2009 stamp', 'Dorman HELP AC Bypass Pulley 34218 Dorman Modern vehicles often use one serpentine belt to drive all accessories, including the water pump, alternator, power steering and A/C compressor. Due to the high cost of A/C compressor replacement, a vehicle can be rendered undriveable if the compressor seizes and causes the belt to break. If a vehicle has high mileage, it may not be economical for a customer to replace the compressor when the cost of the repair approaches the value of the vehicle. An A/C bypass pulley provides an option to keep the vehicle running. An economical way to get a high mileage vehicle back on the road when an air conditioning compressor seizes\\nA quality part backed by a Dorman limited lifetime warranty\\nAn economical way to get a high mileage vehicle back on the road when an air conditioning compressor seizes\\nA quality part backed by a Dorman limited lifetime warranty\\nMeets or exceeds OE specifications for fit, form and function\\nA quality part backed by a Dorman limited lifetime warranty', \"Under Cover Visit Amazon's MaryJanice Davidson Page MaryJanice Davidson is the New York Times bestselling author of the Undead novels featuring Betsy Taylor; Derik's Bane, and the new young adult novels featuring Jennifer Scales, written with her husband, Anthony Alongi, among other titles. \", \"Die heimliche Tochter (German Edition) - Kindle edition Visit Amazon's Georgiana Rose Page  \", 'UBIZ220-New Zinsco QC20 Replacement. Two Pole 20 Amp Thick Series Manufactured by Connecticut Electric. Connecticut Electric Connecticut Electric manufactures new high quality replacement circuit breakers for Zinsco load centers that accept Zinsco QC style circuit breakers. Circuit breakers should only be replaced with new tested and Safety Agency Listed circuit breakers, never used or refurbished ones.\\nThis 120/240 volt Zinsco 20 amp, double pole, thick series replacement circuit breaker is used in a Zinsco load center. Typical applications, up to 8,800 watts, include electric baseboard heat, fan forced heat and 240 volt air conditioners (up to 23,000 BTU). These are only suggested applications and cannot assure compliance with all local codes. Please check local electrical codes before starting any electrical project. Replacement Zinsco circuit breaker manufactured new by Connecticut Electric\\nFor use in Zinsco circuit breaker panels\\nIntertek ETL Listing to UL Standard 489 for US and Canada-Molded case circuit breakers\\n10,000 AIC\\n20 Amp\\n120/240 VAC 2 Pole breaker in a 1 1/2\" body\\nPlug in type\\nMay also be branded as Sylvania (ie Sylvania QC20)', nan, 'Eye Ride Over Glass Goggles (Black/Clear) Eye Ride Eye Ride Motorwear OGG (Over Glass Goggle) - Black/Clear Goggle fits over small reading glasses, and feature a fully-adjustable elastic goggle strap. Double tiered foam padding adds to the long-wear comfort of this goggle. Shatter-resistant polycarbonate lenses are made from the same material used in bullet-resistant automotive glass and safety glasses. A diamond blade and grinding wheel are used to cut and polish the lenses. The lenses are treated with a scratch-resistant coating. The glasses and goggles offer a UV400 rating. Glasses come with the features such as anti-fog coating, flash mirror finishes, and soft-touch frames. Fits over small reading glasses\\nFeatures a fully-adjustable elastic goggle strap\\nDouble tiered foam padding for extra comfort\\nShatter-resistant polycarbonate lenses are made from the same material used in bullet-resistant automotive glass and safety glasses\\nIncludes carrying pouch', '925 Sterling Silver Trinity Cross Pendant Unknown This elegant Celtic Cross pendant would be perfect as a stand-alone medallion on a chain or necklace. Hand crafted from 925 sterling silver and given a polish finish to ensure a lasting shine, this piece of traditional Celtic jewelry has been made in the U.S.A. at our state-of-the-art Los Angeles factory. endless trinity knot designed Celtic cross bracelet charm or necklace pendant\\ncrafted with the finest .925 sterling silver\\ncomes with free special gift packaging\\nmade in the USA yet offered at factory-direct jewelry price\\nships within 24 hours from the manufacturer directly to the customers themselves\\nPackage Dimensions:\\n                    \\n2.2 x 2 x 0.8 inches\\nShipping Weight:\\n                    \\n0.32 ounces (View shipping rates and policies)']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ipdb> foo=tokenizer(txt[5:10], truncation=True, padding='max_length', max_length=max_seq_length)\n",
      "*** TypeError: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
      "ipdb> txt[5:10]\n",
      "[\"Die heimliche Tochter (German Edition) - Kindle edition Visit Amazon's Georgiana Rose Page  \", 'UBIZ220-New Zinsco QC20 Replacement. Two Pole 20 Amp Thick Series Manufactured by Connecticut Electric. Connecticut Electric Connecticut Electric manufactures new high quality replacement circuit breakers for Zinsco load centers that accept Zinsco QC style circuit breakers. Circuit breakers should only be replaced with new tested and Safety Agency Listed circuit breakers, never used or refurbished ones.\\nThis 120/240 volt Zinsco 20 amp, double pole, thick series replacement circuit breaker is used in a Zinsco load center. Typical applications, up to 8,800 watts, include electric baseboard heat, fan forced heat and 240 volt air conditioners (up to 23,000 BTU). These are only suggested applications and cannot assure compliance with all local codes. Please check local electrical codes before starting any electrical project. Replacement Zinsco circuit breaker manufactured new by Connecticut Electric\\nFor use in Zinsco circuit breaker panels\\nIntertek ETL Listing to UL Standard 489 for US and Canada-Molded case circuit breakers\\n10,000 AIC\\n20 Amp\\n120/240 VAC 2 Pole breaker in a 1 1/2\" body\\nPlug in type\\nMay also be branded as Sylvania (ie Sylvania QC20)', nan, 'Eye Ride Over Glass Goggles (Black/Clear) Eye Ride Eye Ride Motorwear OGG (Over Glass Goggle) - Black/Clear Goggle fits over small reading glasses, and feature a fully-adjustable elastic goggle strap. Double tiered foam padding adds to the long-wear comfort of this goggle. Shatter-resistant polycarbonate lenses are made from the same material used in bullet-resistant automotive glass and safety glasses. A diamond blade and grinding wheel are used to cut and polish the lenses. The lenses are treated with a scratch-resistant coating. The glasses and goggles offer a UV400 rating. Glasses come with the features such as anti-fog coating, flash mirror finishes, and soft-touch frames. Fits over small reading glasses\\nFeatures a fully-adjustable elastic goggle strap\\nDouble tiered foam padding for extra comfort\\nShatter-resistant polycarbonate lenses are made from the same material used in bullet-resistant automotive glass and safety glasses\\nIncludes carrying pouch', '925 Sterling Silver Trinity Cross Pendant Unknown This elegant Celtic Cross pendant would be perfect as a stand-alone medallion on a chain or necklace. Hand crafted from 925 sterling silver and given a polish finish to ensure a lasting shine, this piece of traditional Celtic jewelry has been made in the U.S.A. at our state-of-the-art Los Angeles factory. endless trinity knot designed Celtic cross bracelet charm or necklace pendant\\ncrafted with the finest .925 sterling silver\\ncomes with free special gift packaging\\nmade in the USA yet offered at factory-direct jewelry price\\nships within 24 hours from the manufacturer directly to the customers themselves\\nPackage Dimensions:\\n                    \\n2.2 x 2 x 0.8 inches\\nShipping Weight:\\n                    \\n0.32 ounces (View shipping rates and policies)']\n",
      "ipdb> print(txt[5:10])\n",
      "[\"Die heimliche Tochter (German Edition) - Kindle edition Visit Amazon's Georgiana Rose Page  \", 'UBIZ220-New Zinsco QC20 Replacement. Two Pole 20 Amp Thick Series Manufactured by Connecticut Electric. Connecticut Electric Connecticut Electric manufactures new high quality replacement circuit breakers for Zinsco load centers that accept Zinsco QC style circuit breakers. Circuit breakers should only be replaced with new tested and Safety Agency Listed circuit breakers, never used or refurbished ones.\\nThis 120/240 volt Zinsco 20 amp, double pole, thick series replacement circuit breaker is used in a Zinsco load center. Typical applications, up to 8,800 watts, include electric baseboard heat, fan forced heat and 240 volt air conditioners (up to 23,000 BTU). These are only suggested applications and cannot assure compliance with all local codes. Please check local electrical codes before starting any electrical project. Replacement Zinsco circuit breaker manufactured new by Connecticut Electric\\nFor use in Zinsco circuit breaker panels\\nIntertek ETL Listing to UL Standard 489 for US and Canada-Molded case circuit breakers\\n10,000 AIC\\n20 Amp\\n120/240 VAC 2 Pole breaker in a 1 1/2\" body\\nPlug in type\\nMay also be branded as Sylvania (ie Sylvania QC20)', nan, 'Eye Ride Over Glass Goggles (Black/Clear) Eye Ride Eye Ride Motorwear OGG (Over Glass Goggle) - Black/Clear Goggle fits over small reading glasses, and feature a fully-adjustable elastic goggle strap. Double tiered foam padding adds to the long-wear comfort of this goggle. Shatter-resistant polycarbonate lenses are made from the same material used in bullet-resistant automotive glass and safety glasses. A diamond blade and grinding wheel are used to cut and polish the lenses. The lenses are treated with a scratch-resistant coating. The glasses and goggles offer a UV400 rating. Glasses come with the features such as anti-fog coating, flash mirror finishes, and soft-touch frames. Fits over small reading glasses\\nFeatures a fully-adjustable elastic goggle strap\\nDouble tiered foam padding for extra comfort\\nShatter-resistant polycarbonate lenses are made from the same material used in bullet-resistant automotive glass and safety glasses\\nIncludes carrying pouch', '925 Sterling Silver Trinity Cross Pendant Unknown This elegant Celtic Cross pendant would be perfect as a stand-alone medallion on a chain or necklace. Hand crafted from 925 sterling silver and given a polish finish to ensure a lasting shine, this piece of traditional Celtic jewelry has been made in the U.S.A. at our state-of-the-art Los Angeles factory. endless trinity knot designed Celtic cross bracelet charm or necklace pendant\\ncrafted with the finest .925 sterling silver\\ncomes with free special gift packaging\\nmade in the USA yet offered at factory-direct jewelry price\\nships within 24 hours from the manufacturer directly to the customers themselves\\nPackage Dimensions:\\n                    \\n2.2 x 2 x 0.8 inches\\nShipping Weight:\\n                    \\n0.32 ounces (View shipping rates and policies)']\n",
      "ipdb> foo=tokenizer(txt[:7], truncation=True, padding='max_length', max_length=max_seq_length)\n",
      "ipdb> foo=tokenizer(txt[9:10], truncation=True, padding='max_length', max_length=max_seq_length)\n",
      "ipdb> txt[8]\n",
      "'Eye Ride Over Glass Goggles (Black/Clear) Eye Ride Eye Ride Motorwear OGG (Over Glass Goggle) - Black/Clear Goggle fits over small reading glasses, and feature a fully-adjustable elastic goggle strap. Double tiered foam padding adds to the long-wear comfort of this goggle. Shatter-resistant polycarbonate lenses are made from the same material used in bullet-resistant automotive glass and safety glasses. A diamond blade and grinding wheel are used to cut and polish the lenses. The lenses are treated with a scratch-resistant coating. The glasses and goggles offer a UV400 rating. Glasses come with the features such as anti-fog coating, flash mirror finishes, and soft-touch frames. Fits over small reading glasses\\nFeatures a fully-adjustable elastic goggle strap\\nDouble tiered foam padding for extra comfort\\nShatter-resistant polycarbonate lenses are made from the same material used in bullet-resistant automotive glass and safety glasses\\nIncludes carrying pouch'\n",
      "ipdb> foo=tokenizer(txt[8:10], truncation=True, padding='max_length', max_length=max_seq_length)\n",
      "ipdb> foo=tokenizer(txt[7:10], truncation=True, padding='max_length', max_length=max_seq_length)\n",
      "*** AssertionError: text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).\n",
      "ipdb> txt[7]\n",
      "nan\n",
      "ipdb> q\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcmodel = PCModel(\n",
    "    model_name_or_path=args.model_name_or_path,\n",
    "    num_classes= data_module.num_classes,\n",
    "    learning_rate=args.learning_rate,\n",
    "    adam_beta1=args.adam_beta1,\n",
    "    adam_beta2=args.adam_beta2,\n",
    "    adam_epsilon=args.adam_epsilon,\n",
    ")\n",
    "pcmodel.prepare_data()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dl = data_module.train_dataloader()\n",
    "for dat in dl:\n",
    "    input_ids, attention_mask, ys = dat\n",
    "    break\n",
    "\n",
    "logits = pcmodel(input_ids, attention_mask)\n",
    "\n",
    "accu = ((torch.sigmoid(logits)>0.5).int()==ys).float().mean()\n",
    "accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PCDataModule' object has no attribute 'eval_dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-f2bf387936ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdat\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-187224dc696c>\u001b[0m in \u001b[0;36mval_dataloader\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         return DataLoader(\n\u001b[0;32m---> 84\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_batch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PCDataModule' object has no attribute 'eval_dataset'"
     ]
    }
   ],
   "source": [
    "dl = data_module.val_dataloader()\n",
    "n=0\n",
    "for dat in dl:\n",
    "    input_ids, attention_mask, ys = dat\n",
    "\n",
    "    logits = pcmodel(input_ids, attention_mask)\n",
    "\n",
    "    accu = ((torch.sigmoid(logits)>0.5).int()==ys).float().mean()\n",
    "    print(accu)\n",
    "    n+=1\n",
    "    if n>5: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.seed_everything(1234)\n",
    "# trainer = pl.Trainer.from_argparse_args(args)\n",
    "# trainer = pl.Trainer.from_argparse_args(args, fast_dev_run=True)\n",
    "# trainer = pl.Trainer.from_argparse_args(args, limit_train_batches=10, limit_val_batches=5, gpus=1)\n",
    "# trainer = pl.Trainer.from_argparse_args(args, gpus=1)\n",
    "trainer = pl.Trainer.from_argparse_args(args, \n",
    "#                                         limit_train_batches=10, limit_val_batches=5, \n",
    "                                        max_epochs=1,\n",
    "#                                         fast_dev_run=True, \n",
    "                                        log_gpu_memory=True, gpus=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(pcmodel, data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls {p_out}/lightning_logs/"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!tensorboard --logdir {p_out}/lightning_logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = data_module.val_dataloader()\n",
    "n=0\n",
    "for dat in dl:\n",
    "    input_ids, attention_mask, ys = dat\n",
    "\n",
    "    logits = pcmodel(input_ids, attention_mask)\n",
    "\n",
    "    accu = ((torch.sigmoid(logits)>0.5).int()==ys).float().mean()\n",
    "    print(accu)\n",
    "    n+=1\n",
    "    if n>5: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "product-category",
   "language": "python",
   "name": "product-category"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
